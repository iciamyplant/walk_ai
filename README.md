# walk_ai




## 1.  Reinforcement Learning
#### a. D√©finition

Branche de l'intelligence artificielle o√π un agent apprend √† prendre des d√©cisions en interagissant avec son environnement. C'est √† dire que l'agent prend des actions dans un environnement et re√ßoit des r√©compenses ou des p√©nalit√©s en fonction de ces actions, sachant que l‚Äôobjectif est de trouver un mod√®le d‚Äôaction appropri√© qui maximiserait la r√©compense cumul√©e totale de l‚Äôagent. ‚áí il doit donc apprendre quelle action prendre dans quelle situation pour maximiser sa r√©compense globale. Contrairement √† d'autres formes d'apprentissage supervis√© ou non supervis√©, le renforcement ne n√©cessite pas d'exemples √©tiquet√©s de donn√©es en entr√©e ou de r√©sultats souhait√©s.

<p align="center">
<img width="700" src="https://github.com/iciamyplant/walk_ai/assets/57531966/a101cebd-16b7-4251-9d3e-94a4c9a0a694">
<p align="center">

Concept
- un agent, repr√©sente celui qui apprend
- takes actions at each of time t : At
- the action is received by the environnement
- qui l'utilise pour produire une r√©compense Rt+1, et un √©tat √† ce moment du temps St+1
- ceci est renvoy√© √† l'agent pour que la prochaine action soit d√©cid√©e
- le processus se rep√®te

<p align="center">
<img width="400" src="https://github.com/iciamyplant/walk_ai/assets/57531966/833def76-8757-4140-a907-4e5eb55c2229">
<p align="center">

**Markov Decision Processes(MDPs)** = are mathematical frameworks to describe an environment in RL and almost all RL problems can be formulated using MDPs. An MDP consists of a set of finite environment states S, a set of possible actions A(s) in each state, a real valued reward function R(s) and a transition model P(s‚Äô, s | a). However, real world environments are more likely to lack any prior knowledge of environment dynamics. Model-free RL methods come handy in such cases.

time is discrete : t=[0,1,2,3,...]
souvent les √©tats, les actions et les r√©compenses sont d√©finis, et l'agent apprend √† travers l'exploration et l'exploitation comment agir de mani√®re optimale dans ces √©tats pour maximiser les r√©compenses


#### b. Cas d'utilisation

Adapt√©s dans les situations o√π il faut prendre des d√©cisions les unes apr√®s les autres dans un environnement incertain pour atteindre des objectifs sp√©cifiques. Applications dans de nombreux domaines :
- Les jeux : Deepmind impressive version of RL : Alpha zero devenant l'un des meilleurs moteurs d'√©checs utilisant seulement quelques heures de jeu, Muzero atteignant un niveau de performance similaire mais sur un esemble de taches bcp plus diversifi√© : atari games, go. Mais aussi AlphaGo de Deepmind. PrefixRL, Nvidia a annonc√© avoir utilis√© du deep RL pour concevoir des circuits airthm√©tiques plus approfondis. Le RL a √©t√© largement utilis√© pour cr√©er des agents autonomes capables de jouer √† des jeux vid√©o comme Dota 2, StarCraft II, ou des jeux de plateau comme Go, √©checs et dame
- Robotique : Les robots autonomes peuvent apprendre √† manipuler des objets, √† marcher, √† naviguer dans des environnements complexes et √† accomplir diverses t√¢ches en utilisant le RL pour s'adapter √† des conditions changeantes.
- Publicit√© en ligne : Les algorithmes de RL sont souvent utilis√©s pour optimiser les campagnes publicitaires en ligne, en ajustant dynamiquement les ench√®res et les strat√©gies de diffusion pour maximiser le retour sur investissement. plateforme utilise le RL pour ajuster dynamiquement les ench√®res et les strat√©gies de diffusion des annonces afin d'optimiser les performances de la campagne publicitaire. Les r√©compenses pourraient √™tre bas√©es sur des m√©triques telles que le nombre de clics, le taux de conversion, le chiffre d'affaires g√©n√©r√©, etc. Au fur et √† mesure que de nouvelles donn√©es sont disponibles, l'agent ajuste ses d√©cisions en temps r√©el pour s'adapter aux changements dans l'environnement, tels que les tendances du march√©, la concurrence, ou les pr√©f√©rences des utilisateurs.
- Contr√¥le de la circulation et logistique : Le RL peut √™tre appliqu√© pour optimiser la circulation routi√®re, la gestion des feux de signalisation, la logistique de livraison et la planification des itin√©raires
- La gestion des ressources, la finance...

#### c. Algos 

Le RL fait appel √† une vari√©t√© d'algorithmes et de techniques informatiques pour permettre √† un agent d'apprendre √† prendre des d√©cisions dans un environnement donn√©, quelques unes de ces m√©thodes :

|Method|Definition|Comments
|-----|-----|-----|
|Q-Learning|C'est l'un des algorithmes les plus fondamentaux du RL. Il est utilis√© pour apprendre une fonction d'√©valuation de l'action appel√©e fonction Q, qui indique la valeur d'une action dans un √©tat donn√©||
|SARSA (State-Action-Reward-State-Action)|C'est un autre algorithme bas√© sur la programmation dynamique qui est utilis√© pour apprendre une politique d'action optimale dans un environnement de RL|La programmation dynamique est une m√©thode pour r√©soudre des probl√®mes de d√©cision s√©quentielle en d√©composant le probl√®me en sous-probl√®mes plus simples et en utilisant une relation de r√©currence entre ces sous-probl√®mes. Elle est souvent utilis√©e pour r√©soudre des MDPs discrets et d√©terministes|
|M√©thodes de Monte Carlo|m√©thodes sont utilis√©es pour estimer la valeur d'une politique en √©chantillonnant des trajectoires compl√®tes √† partir de l'environnement. Elles sont particuli√®rement utiles lorsque les mod√®les de transition d'√©tat ne sont pas connus||
|td learning|TD Learning est une famille d'algorithmes d'apprentissage par renforcement qui mettent √† jour les estimations de valeur des √©tats ou des actions en utilisant une combinaison de la m√©thode de Monte Carlo et de la m√©thode de la diff√©rence temporelle. Les m√©thodes TD sont souvent utilis√©es lorsque les transitions d'√©tat sont partiellement observables||
|R√©seaux de neurones profonds (Deep Q-Networks, DQN)|Les DQN sont une m√©thode qui utilise des r√©seaux de neurones profonds pour apprendre directement √† partir de donn√©es brutes en entr√©e, tels que des pixels dans une image||
|Policy Gradients|Cette approche consiste √† optimiser directement la politique d'action en utilisant des m√©thodes de descente de gradient. Elle est souvent utilis√©e lorsque les actions sont continues ou lorsque l'approximation de la fonction de valeur est difficile||
|...|||

## 2.  Implementing Snake with Q-learning
- Environment ‚Äî Physical world in which the agent operates
- State ‚Äî Current situation of the agent
- Reward ‚Äî Feedback from the environment
- Policy ‚Äî Method to map agent‚Äôs state to actions
- Value ‚Äî Future reward that an agent would receive by taking an action in a particular state

[https://www.youtube.com/watch?v=L8ypSXwyBds](snake tuorial with neural networks)
[https://www.youtube.com/watch?v=ZhoIgo3qqLU](frozen lake gym tuto)






## 2. Humanoide 2D processus d'optimisation ARS

Augmented Random Search Algorithm
- ARS is a latest A.I algorithm proposed in march 2018 dans cet article : [paper](https://arxiv.org/pdf/1803.07055.pdf)
- As in every RL problem, the target is to find a policy to maximize the expected reward that the agent might get when following this policy in the given environment (= trouver une politique permettant de maximiser la r√©compense attendue que l'agent pourrait obtenir en suivant cette politique dans l'environnement donn√©)
- It almost 15 times faster than other reinforcement algorithms of 2017.
- ARS is based on shallow learning method
- ARS exemple 2D gym humanoide [ex](https://www.youtube.com/watch?v=TVsPttCWeOo)

La solution propos√©e par l'article consiste √† am√©liorer un algorithme existant appel√© Basic Random Search. Basic random search = L'id√©e de la recherche al√©atoire de base est de choisir une politique pramat√©ris√©e ùúãùúÉ, de choquer (ou de perturber) les param√®tres ùúÉ en appliquant +ùõéùúπ et -ùõéùúπ (o√π ùõé < 1 est un bruit constant et ùúπ est un nombre al√©atoire g√©n√©r√© √† partir d'une distribution normale). Appliquez ensuite les actions bas√©es sur ùúã(ùúÉ+ùõéùúπ) et ùúã(ùúÉ-ùõéùúπ) puis r√©cup√©rez les r√©compenses r(ùúÉ+ùõéùúπ) et r(ùúÉ-ùõéùúπ) r√©sultant de ces actions. Maintenant que nous avons les r√©compenses du ùúÉ perturb√©, calculez la moyenne Œî = 1/N * Œ£[r(ùúÉ+ùõéùúπ) - r(ùúÉ-ùõéùúπ)]ùúπ pour tous les ùúπ et on met √† jour les param√®tres ùúÉ en utilisant Œî et un taux d'apprentissage ùù∞.ùúÉ ≤‚Å∫¬π = ùúÉ ≤ + ùù∞.Œî



Il poss√®de une couche d'entr√©e qui accepte un vecteur de l'√©tat de l'environnement
puis, apr√®s les avoir multipli√©s par les poids et pass√© par une fonction d'activation
il donne une sortie √† l'agent sur l'action qu'il doit effectuer sur l'environnement

Il est diff√©rent des autres mod√®les d'apprentissage par renforcement car il n'a pas de r√©seau neuronal profond entre ses couches d'entr√©e et de sortie. De plus, il ne fonctionne pas sur l'espace d'action, il fonctionne sur l'espace politique.

ARS utilise une approche diff√©rente pour optimiser son poids. Contrairement √† d‚Äôautres algorithmes, il utilise une approche par diff√©rences finies pour optimiser les poids. f'(x) = summation(f(a+h)-f(a)) o√π h est une tr√®s petite pertubation.


## 3. Humanoide 3D avec processus PPOT







-------

### **Sources**
- [Slides on RL by Pieter Abbeel and John Schulman - Open AI/ Berkeley AI Research Lab](https://people.eecs.berkeley.edu/~pabbeel/nips-tutorial-policy-optimization-Schulman-Abbeel.pdf)
- [Cours videos en 10 lessons by David Silver DeepMind](https://www.youtube.com/watch?v=2pWv7GOvuf0)
- [Tutorial implementing Neural Network Policy Gradients for Pong game](https://karpathy.github.io/2016/05/31/rl/)
- [DeepMind Lab = open source 3D game-like platform created for agent-based AI research with rich simulated environments](https://deepmind.google/discover/blog/open-sourcing-deepmind-lab/)
- [Malmo, Microsoft = AI experimentation platform for supporting fundamental research in AI](https://www.microsoft.com/en-us/research/project/project-malmo/)
- [Gym OpenAI](https://www.gymlibrary.dev/index.html)

Intall VM
- ajouter Vbox Guest additions dans stockage
- config > general > avanc√© > presse-papier bidirectionnel
- sudo apt install linux-headers-$(uname -r) build-essential dkms
- redemarrer la vm
- [tuto](https://www.youtube.com/watch?v=MI1THQJFZXY)






