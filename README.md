# walk_ai

## 1.  Reinforcement Learning and Q Learning

Deepmind impressive version of RL : Alpha zero devenant l'un des meilleurs moteurs d'Ã©checs utilisant seulement quelques heures de jeu, Muzero atteignant un niveau de performance similaire mais sur un esemble de taches bcp plus diversifiÃ© : atari games, go.
PrefixRL, Nvidia a annoncÃ© avoir utilisÃ© du deep RL pour concevoir des circuits airthmÃ©tiques plus approfondis.
- the bellman equations, dynamic programming, genereliazed policy iteration
- monte carlo and off policy methods
- td learning, sarsa, and q learning
- function approximation
- policy gradient methods

Concept
- un agent, reprÃ©sente celui qui apprend
- takes actions at each of time t : At
- the action is received by the environnement
- qui l'utilise pour produire une rÃ©compense Rt+1, et un Ã©tat Ã  ce moment du temps St+1
- ceci est renvoyÃ© Ã  l'agent pour que la prochaine action soit dÃ©cidÃ©e
- le processus se repÃ¨te

time is discrete : t=[0,1,2,3,...]

reinforcement learning
q learning
Snake Game Example

## 2. Humanoide 2D processus d'optimisation ARS

Augmented Random Search Algorithm
- ARS is a latest A.I algorithm proposed in march 2018 dans cet article : [paper](https://arxiv.org/pdf/1803.07055.pdf)
- As in every RL problem, the target is to find a policy to maximize the expected reward that the agent might get when following this policy in the given environment (= trouver une politique permettant de maximiser la rÃ©compense attendue que l'agent pourrait obtenir en suivant cette politique dans l'environnement donnÃ©)
- It almost 15 times faster than other reinforcement algorithms of 2017.
- ARS is based on shallow learning method
- ARS exemple 2D gym humanoide [ex](https://www.youtube.com/watch?v=TVsPttCWeOo)

La solution proposÃ©e par l'article consiste Ã  amÃ©liorer un algorithme existant appelÃ© Basic Random Search. Basic random search = L'idÃ©e de la recherche alÃ©atoire de base est de choisir une politique pramatÃ©risÃ©e ğœ‹ğœƒ, de choquer (ou de perturber) les paramÃ¨tres ğœƒ en appliquant +ğ›ğœ¹ et -ğ›ğœ¹ (oÃ¹ ğ› < 1 est un bruit constant et ğœ¹ est un nombre alÃ©atoire gÃ©nÃ©rÃ© Ã  partir d'une distribution normale). Appliquez ensuite les actions basÃ©es sur ğœ‹(ğœƒ+ğ›ğœ¹) et ğœ‹(ğœƒ-ğ›ğœ¹) puis rÃ©cupÃ©rez les rÃ©compenses r(ğœƒ+ğ›ğœ¹) et r(ğœƒ-ğ›ğœ¹) rÃ©sultant de ces actions. Maintenant que nous avons les rÃ©compenses du ğœƒ perturbÃ©, calculez la moyenne Î” = 1/N * Î£[r(ğœƒ+ğ›ğœ¹) - r(ğœƒ-ğ›ğœ¹)]ğœ¹ pour tous les ğœ¹ et on met Ã  jour les paramÃ¨tres ğœƒ en utilisant Î” et un taux d'apprentissage ğ°.ğœƒÊ²âºÂ¹ = ğœƒÊ² + ğ°.Î”



Il possÃ¨de une couche d'entrÃ©e qui accepte un vecteur de l'Ã©tat de l'environnement
puis, aprÃ¨s les avoir multipliÃ©s par les poids et passÃ© par une fonction d'activation
il donne une sortie Ã  l'agent sur l'action qu'il doit effectuer sur l'environnement

Il est diffÃ©rent des autres modÃ¨les d'apprentissage par renforcement car il n'a pas de rÃ©seau neuronal profond entre ses couches d'entrÃ©e et de sortie. De plus, il ne fonctionne pas sur l'espace d'action, il fonctionne sur l'espace politique.

ARS utilise une approche diffÃ©rente pour optimiser son poids. Contrairement Ã  dâ€™autres algorithmes, il utilise une approche par diffÃ©rences finies pour optimiser les poids. f'(x) = summation(f(a+h)-f(a)) oÃ¹ h est une trÃ¨s petite pertubation.


## 3. Humanoide 3D avec processus PPOT







-------
Monte carlo method



Intall VM
- ajouter Vbox Guest additions dans stockage
- config > general > avancÃ© > presse-papier bidirectionnel
- sudo apt install linux-headers-$(uname -r) build-essential dkms
- redemarrer la vm
- [tuto](https://www.youtube.com/watch?v=MI1THQJFZXY)






