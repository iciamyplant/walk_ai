# walk_ai




## 1.  Reinforcement Learning
#### a. D√©finition

Branche de l'intelligence artificielle o√π un agent apprend √† prendre des d√©cisions en interagissant avec son environnement. C'est √† dire que l'agent prend des actions dans un environnement et re√ßoit des r√©compenses ou des p√©nalit√©s en fonction de ces actions, sachant que l‚Äôobjectif est de trouver un mod√®le d‚Äôaction appropri√© qui maximiserait la r√©compense cumul√©e totale de l‚Äôagent. ‚áí il doit donc apprendre quelle action prendre dans quelle situation pour maximiser sa r√©compense globale. Contrairement √† d'autres formes d'apprentissage supervis√© ou non supervis√©, le renforcement ne n√©cessite pas d'exemples √©tiquet√©s de donn√©es en entr√©e ou de r√©sultats souhait√©s.

<p align="center">
<img width="700" src="https://github.com/iciamyplant/walk_ai/assets/57531966/a101cebd-16b7-4251-9d3e-94a4c9a0a694">
<p align="center">

Concept
- un agent, repr√©sente celui qui apprend
- takes actions at each of time t : At
- the action is received by the environnement
- qui l'utilise pour produire une r√©compense Rt+1, et un √©tat √† ce moment du temps St+1
- ceci est renvoy√© √† l'agent pour que la prochaine action soit d√©cid√©e
- le processus se rep√®te

time is discrete : t=[0,1,2,3,...]
souvent les √©tats, les actions et les r√©compenses sont d√©finis, et l'agent apprend √† travers l'exploration et l'exploitation comment agir de mani√®re optimale dans ces √©tats pour maximiser les r√©compenses


#### b. Cas d'utilisation

Adapt√©s dans les situations o√π il faut prendre des d√©cisions les unes apr√®s les autres dans un environnement incertain pour atteindre des objectifs sp√©cifiques. Applications dans de nombreux domaines :
- Les jeux : Deepmind impressive version of RL : Alpha zero devenant l'un des meilleurs moteurs d'√©checs utilisant seulement quelques heures de jeu, Muzero atteignant un niveau de performance similaire mais sur un esemble de taches bcp plus diversifi√© : atari games, go. Mais aussi AlphaGo de Deepmind. PrefixRL, Nvidia a annonc√© avoir utilis√© du deep RL pour concevoir des circuits airthm√©tiques plus approfondis. Le RL a √©t√© largement utilis√© pour cr√©er des agents autonomes capables de jouer √† des jeux vid√©o comme Dota 2, StarCraft II, ou des jeux de plateau comme Go, √©checs et dame
- Robotique : Les robots autonomes peuvent apprendre √† manipuler des objets, √† marcher, √† naviguer dans des environnements complexes et √† accomplir diverses t√¢ches en utilisant le RL pour s'adapter √† des conditions changeantes.
- Publicit√© en ligne : Les algorithmes de RL sont souvent utilis√©s pour optimiser les campagnes publicitaires en ligne, en ajustant dynamiquement les ench√®res et les strat√©gies de diffusion pour maximiser le retour sur investissement. plateforme utilise le RL pour ajuster dynamiquement les ench√®res et les strat√©gies de diffusion des annonces afin d'optimiser les performances de la campagne publicitaire. Les r√©compenses pourraient √™tre bas√©es sur des m√©triques telles que le nombre de clics, le taux de conversion, le chiffre d'affaires g√©n√©r√©, etc. Au fur et √† mesure que de nouvelles donn√©es sont disponibles, l'agent ajuste ses d√©cisions en temps r√©el pour s'adapter aux changements dans l'environnement, tels que les tendances du march√©, la concurrence, ou les pr√©f√©rences des utilisateurs.
- Contr√¥le de la circulation et logistique : Le RL peut √™tre appliqu√© pour optimiser la circulation routi√®re, la gestion des feux de signalisation, la logistique de livraison et la planification des itin√©raires
- La gestion des ressources, la finance...

#### c. Algos 

Le RL fait appel √† une vari√©t√© d'algorithmes et de techniques informatiques pour permettre √† un agent d'apprendre √† prendre des d√©cisions dans un environnement donn√©, quelques unes de ces m√©thodes :

|Method|Definition|
|-----|-----|
|Q-Learning|C'est l'un des algorithmes les plus fondamentaux du RL. Il est utilis√© pour apprendre une fonction d'√©valuation de l'action appel√©e fonction Q, qui indique la valeur d'une action dans un √©tat donn√©|
|SARSA (State-Action-Reward-State-Action)|C'est un autre algorithme bas√© sur la programmation dynamique qui est utilis√© pour apprendre une politique d'action optimale dans un environnement de RL|













## 2. Humanoide 2D processus d'optimisation ARS

Augmented Random Search Algorithm
- ARS is a latest A.I algorithm proposed in march 2018 dans cet article : [paper](https://arxiv.org/pdf/1803.07055.pdf)
- As in every RL problem, the target is to find a policy to maximize the expected reward that the agent might get when following this policy in the given environment (= trouver une politique permettant de maximiser la r√©compense attendue que l'agent pourrait obtenir en suivant cette politique dans l'environnement donn√©)
- It almost 15 times faster than other reinforcement algorithms of 2017.
- ARS is based on shallow learning method
- ARS exemple 2D gym humanoide [ex](https://www.youtube.com/watch?v=TVsPttCWeOo)

La solution propos√©e par l'article consiste √† am√©liorer un algorithme existant appel√© Basic Random Search. Basic random search = L'id√©e de la recherche al√©atoire de base est de choisir une politique pramat√©ris√©e ùúãùúÉ, de choquer (ou de perturber) les param√®tres ùúÉ en appliquant +ùõéùúπ et -ùõéùúπ (o√π ùõé < 1 est un bruit constant et ùúπ est un nombre al√©atoire g√©n√©r√© √† partir d'une distribution normale). Appliquez ensuite les actions bas√©es sur ùúã(ùúÉ+ùõéùúπ) et ùúã(ùúÉ-ùõéùúπ) puis r√©cup√©rez les r√©compenses r(ùúÉ+ùõéùúπ) et r(ùúÉ-ùõéùúπ) r√©sultant de ces actions. Maintenant que nous avons les r√©compenses du ùúÉ perturb√©, calculez la moyenne Œî = 1/N * Œ£[r(ùúÉ+ùõéùúπ) - r(ùúÉ-ùõéùúπ)]ùúπ pour tous les ùúπ et on met √† jour les param√®tres ùúÉ en utilisant Œî et un taux d'apprentissage ùù∞.ùúÉ ≤‚Å∫¬π = ùúÉ ≤ + ùù∞.Œî



Il poss√®de une couche d'entr√©e qui accepte un vecteur de l'√©tat de l'environnement
puis, apr√®s les avoir multipli√©s par les poids et pass√© par une fonction d'activation
il donne une sortie √† l'agent sur l'action qu'il doit effectuer sur l'environnement

Il est diff√©rent des autres mod√®les d'apprentissage par renforcement car il n'a pas de r√©seau neuronal profond entre ses couches d'entr√©e et de sortie. De plus, il ne fonctionne pas sur l'espace d'action, il fonctionne sur l'espace politique.

ARS utilise une approche diff√©rente pour optimiser son poids. Contrairement √† d‚Äôautres algorithmes, il utilise une approche par diff√©rences finies pour optimiser les poids. f'(x) = summation(f(a+h)-f(a)) o√π h est une tr√®s petite pertubation.


## 3. Humanoide 3D avec processus PPOT







-------
Monte carlo method



Intall VM
- ajouter Vbox Guest additions dans stockage
- config > general > avanc√© > presse-papier bidirectionnel
- sudo apt install linux-headers-$(uname -r) build-essential dkms
- redemarrer la vm
- [tuto](https://www.youtube.com/watch?v=MI1THQJFZXY)






